{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try_dl_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_k91V7UYQl6"
      },
      "source": [
        "!pip install wandb\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from tqdm.auto import tqdm\n",
        "import tensorflow as tf\n",
        "import wandb\n",
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjPTg70-YXjM"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESi2E1-zC6GV"
      },
      "source": [
        "def load_fashion_mnist(return_images=False):\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  train_shuffler = np.random.shuffle(np.arange(60000))\n",
        "  x_train, y_train = x_train[train_shuffler][0], y_train[train_shuffler][0]\n",
        "\n",
        "  test_shuffler = np.random.shuffle(np.arange(10000))\n",
        "  x_test, y_test = x_test[test_shuffler][0], y_test[test_shuffler][0]\n",
        "\n",
        "  x_train = np.array(x_train/255).astype('float32')\n",
        "  x_test = np.array(x_test/255).astype('float32')\n",
        "\n",
        "  x_train, x_val = x_train[:54000], x_train[54000:]\n",
        "  y_train, y_val = y_train[:54000], y_train[54000:]\n",
        "\n",
        "\n",
        "  if (return_images==False):\n",
        "    return {\n",
        "        'train': {\n",
        "            'X': x_train.reshape([-1, 784]),\n",
        "            'Y': y_train.reshape([54000])\n",
        "        },\n",
        "        'val': {\n",
        "            'X': x_val.reshape([-1, 784]),\n",
        "            'Y': y_val.reshape([6000])\n",
        "        },\n",
        "        'test': {\n",
        "            'X': x_test.reshape([-1, 784]),\n",
        "            'Y': y_test.reshape([10000])\n",
        "        }\n",
        "  }\n",
        "\n",
        "  else :\n",
        "    return {\n",
        "      'train': {\n",
        "          \t'X': x_train,\n",
        "          \t'Y': y_train\n",
        "      },\n",
        "      'val': {\n",
        "            'X': x_val,\n",
        "            'Y': y_val\n",
        "      },\n",
        "      'test': {\n",
        "            'X': x_test,\n",
        "            'Y': y_test\n",
        "      }\n",
        "    }\n",
        "\n",
        "\n",
        "data = load_fashion_mnist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-EXaU4oiZ_"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23nXB0VBojt0"
      },
      "source": [
        "class neural_network:\n",
        "\n",
        "  # constructor function - initializes weights\n",
        "  def __init__(self, dict_layers, initializer):\n",
        "\n",
        "    self.weights_list = []\n",
        "    self.biases_list = []\n",
        "    self.dict_layers= dict_layers\n",
        "\n",
        "    self.weights_list, self.biases_list = wandb_initializer(dict_layers, self.weights_list, self.biases_list, initializer)\n",
        "\n",
        "  # function to compute forward propogation\n",
        "  def forward_prop(self, W, b, X, Y, activation_func):\n",
        "\n",
        "    A = []\n",
        "    H = []\n",
        "    \n",
        "    H_pre = X\n",
        "    \n",
        "    L = self.dict_layers['num_hidden_layers']\n",
        "\n",
        "    for i in range(L) :\n",
        "      A.append(W[i] @ H_pre + b[i])\n",
        "      H_pre = getattr(activation, activation_func)(A[i])\n",
        "      H.append(H_pre)\n",
        "    \n",
        "    A.append(W[L] @ H_pre + b[L])\n",
        "    \n",
        "    Y_hat = activation.softmax(A[L])\n",
        "    \n",
        "    return {\n",
        "        'A' : A,\n",
        "        'H' : H,\n",
        "        'Y_hat' : Y_hat\n",
        "    }\n",
        "\n",
        "  # helper function to perform forward propogation \n",
        "  def self_forward_prop(self, X, Y, activation_func) :\n",
        "\n",
        "    temp = self.forward_prop(self.weights_list,self.biases_list, X, Y, activation_func)\n",
        "    return temp\n",
        "\n",
        "  # function to perform backward propogration\n",
        "  def back_prop(self, W, b, A, H, Y_hat, X, Y,activation_func):\n",
        "\n",
        "    batch_size = len(Y)\n",
        "    \n",
        "    del_w = []\n",
        "    del_b = []\n",
        "    L = self.dict_layers['num_hidden_layers']\n",
        "    \n",
        "    E = np.zeros(Y_hat.shape)\n",
        "    E[Y,np.arange(batch_size)] = 1\n",
        "    \n",
        "    grad_A = -(E - Y_hat)\n",
        "\n",
        "    for i in range(L,-1,-1) :\n",
        "\n",
        "      temp1 = grad_A.reshape(-1,batch_size)\n",
        "      \n",
        "      if i==0 :\n",
        "        temp2 = X.T\n",
        "      else :\n",
        "        temp2 = H[i-1].reshape((batch_size ,-1))\n",
        "\n",
        "      del_w.append(temp1 @ temp2/batch_size)\n",
        "      del_b.append(grad_A/batch_size)\n",
        "\n",
        "      if(i!=0) :\n",
        "        grad_H = W[i].T @ grad_A      \n",
        "        grad_A = grad_H * getattr(activation,activation_func+'_der')(H[i-1])\n",
        "\n",
        "    for j in range(len(del_b)) :\n",
        "       del_b[j] = np.sum(del_b[j],axis=1)\n",
        "\n",
        "    return {\n",
        "        'dw' : del_w,\n",
        "        'db' : del_b\n",
        "    }\n",
        "\n",
        "  # helper function to perform backward propogation\n",
        "  def self_back_prop(self, A, H, Y_hat, X, Y,activation_func) :\n",
        "    temp = self.back_prop(self.weights_list,self.biases_list, A, H, Y_hat, X, Y, activation_func)\n",
        "    return temp\n",
        "\n",
        "  #  function to compute gradient\n",
        "  def grad_wandb(self, W, b, X, Y,activation_func):\n",
        "\n",
        "    X = X.T.reshape((784,-1))\n",
        "    \n",
        "    temp = self.forward_prop(W, b, X, Y, activation_func)\n",
        "    temp2 = self.back_prop(W, b, temp['A'], temp['H'], temp['Y_hat'], X, Y, activation_func)\n",
        "\n",
        "    return {\n",
        "        'dw' : temp2['dw'],\n",
        "        'db' : temp2['db']\n",
        "    }\n",
        "\n",
        "  # helper function to compute gradient\n",
        "  def self_grad_wandb(self, X, Y, activation_func) :\n",
        "    temp = self.grad_wandb(self.weights_list, self.biases_list, X, Y,activation_func)\n",
        "    return temp\n",
        "\n",
        "  # function to compute predictions\n",
        "  def predict(self, X, activation_func):\n",
        "    X = X.T.reshape((784,-1))\n",
        "    temp = self.forward_prop(self.weights_list,self.biases_list, X, 0, activation_func)\n",
        "    return {\n",
        "      'Y' : np.argmax(temp['Y_hat'],axis=0),\n",
        "      'Y_hat' : temp['Y_hat']\n",
        "    }\n",
        "\n",
        "  # function to update weights and biases\n",
        "  def update_vals(self, dw, db, wd) :\n",
        "    L = len(self.weights_list)\n",
        "    for i in range(L) :\n",
        "      self.weights_list[i] =self.weights_list[i] - dw[L-i-1].reshape(self.weights_list[i].shape) - wd * self.weights_list[i]\n",
        "\n",
        "    #for i in range(len(self.biases_list)) :\n",
        "      self.biases_list[i] =self.biases_list[i] - db[L-i-1].reshape(self.biases_list[i].shape)  \n",
        "##################################################################################\n",
        "class activation:\n",
        "  \n",
        "  @staticmethod\n",
        "  def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "  \n",
        "  @staticmethod\n",
        "  def relu(z):\n",
        "    return (z>0) * z\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_der(z) :\n",
        "    return z * (1-z)\n",
        "  \n",
        "  @staticmethod\n",
        "  def relu_der(z) :\n",
        "    return (z>0)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_der(z):\n",
        "    return 1 - z*z\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x,axis=0)\n",
        "\n",
        "##################################################################################\n",
        "def set_nn_shape(verbose=True, num_hidden_layers=-1, hidden_layer_size=-1):\n",
        "\n",
        "  input_layer_size = 784\n",
        "  hidden_layer_size = hidden_layer_size\n",
        "  num_hidden_layers = num_hidden_layers\n",
        "  output_layer_size = 10\n",
        "  \n",
        "  # input_layer_size = 3\n",
        "  # hidden_layer_size = hidden_layer_size\n",
        "  # num_hidden_layers = num_hidden_layers\n",
        "  # output_layer_size = 2\n",
        "  if (verbose):\n",
        "    print(\"\\nNumber Of Hidden Layers:\")\n",
        "    num_hidden_layers = int(input())\n",
        "\n",
        "    print(\"\\nSize Of Each Hidden Layer:\")\n",
        "    hidden_layer_size = int(input())\n",
        "\n",
        "    print(f\"\\nThe Neural Network Has {num_hidden_layers+2} Layers In Total!\")\n",
        "  \n",
        "  return {\"input_layer_size\": input_layer_size, \"hidden_layer_size\": hidden_layer_size, \"output_layer_size\": output_layer_size, \"num_hidden_layers\": num_hidden_layers}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIFMpZQNItG3"
      },
      "source": [
        "def wandb_initializer(nn_shape, weights_list, biases_list, type='random', mu = 0, sigma = 1):\n",
        "  \n",
        "  # random initialization\n",
        "  if (type=='random'):\n",
        "    initializer = tf.keras.initializers.TruncatedNormal(mean=mu, stddev=sigma)\n",
        "  \n",
        "  # xavier initialization\n",
        "  elif (type=='xavier'):\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "  weights_list.append(initializer(shape=(nn_shape['hidden_layer_size'], nn_shape['input_layer_size'])).numpy())\n",
        "  biases_list.append(initializer(shape=(nn_shape['hidden_layer_size'], 1)).numpy())\n",
        "  for i in range(nn_shape['num_hidden_layers'] - 1):\n",
        "    weights_list.append(initializer(shape=(nn_shape['hidden_layer_size'], nn_shape['hidden_layer_size'])).numpy())\n",
        "    biases_list.append(initializer(shape=(nn_shape['hidden_layer_size'], 1)).numpy())\n",
        "\n",
        "  weights_list.append(initializer(shape=(nn_shape['output_layer_size'], nn_shape['hidden_layer_size'])).numpy())\n",
        "  biases_list.append(initializer(shape=(nn_shape['output_layer_size'], 1)).numpy())\n",
        "\n",
        "  return weights_list, biases_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVpF16BtXqB_"
      },
      "source": [
        "class optimizer:\n",
        "\n",
        "  @staticmethod\n",
        "  def sgd(network, data, config):\n",
        "\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        " \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        X = X_train[k: k+batch_size]\n",
        "        Y = Y_train[k: k+batch_size]\n",
        "        temp = network.self_grad_wandb(X, Y, activation_func)         \n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "        for dd in dw :\n",
        "          dd*= eta\n",
        "        for dd in db :\n",
        "          dd*=eta\n",
        "\n",
        "        network.update_vals(dw, db, lambda_)\n",
        "    \n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def momentum(network, data, config,gamma = 0.9) :\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        "    \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "    dw, db = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    dw.reverse()\n",
        "    db.reverse()\n",
        "\n",
        "    for j in range(len(db)) :\n",
        "      db[j] = db[j].flatten()\n",
        "\n",
        "    for i in range(num_epochs) :\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        X = X_train[k:k+batch_size]\n",
        "        Y = Y_train[k:k+batch_size]\n",
        "        temp = network.self_grad_wandb(X,Y,activation_func)       \n",
        "        for j in range(len(dw)) :\n",
        "          dw[j] += eta*temp['dw'][j]\n",
        "          db[j] += eta*temp['db'][j]\n",
        "\n",
        "        network.update_vals(dw,db, lambda_)\n",
        "        for dd in db :\n",
        "          dd*=gamma\n",
        "        for dd in dw :\n",
        "          dd*=gamma\n",
        "\n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "        \n",
        "  @staticmethod\n",
        "  def NAG(network, data, config,gamma = 0.9) :\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        "    \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    \n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "    v_dw, v_db = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    v_dw.reverse()\n",
        "    v_db.reverse()\n",
        "\n",
        "    for j in range(len(v_db)) :\n",
        "      v_db[j] = v_db[j].flatten()\n",
        "\n",
        "    for i in range(num_epochs) :\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        for j in range(len(v_dw)) :\n",
        "          v_dw[j] = gamma*v_dw[j]\n",
        "          v_db[j] = gamma*v_db[j]\n",
        "\n",
        "        X = X_train[k:k+batch_size]\n",
        "        Y = Y_train[k:k+batch_size]\n",
        "        \n",
        "        W = network.weights_list.copy()\n",
        "        B = network.biases_list.copy()\n",
        "\n",
        "        L = len(W)\n",
        "        for j in range(L) :\n",
        "          W[j] -= v_dw[L-j-1]\n",
        "          B[j] -= v_db[L-j-1].reshape(B[j].shape)\n",
        "\n",
        "        temp = network.grad_wandb(W,B,X,Y,activation_func)  \n",
        "\n",
        "        for j in range(len(v_dw)) :\n",
        "          v_dw[j] += eta*temp['dw'][j]\n",
        "          v_db[j] += eta*temp['db'][j]\n",
        "\n",
        "\n",
        "        network.update_vals(v_dw,v_db,lambda_)\n",
        "\n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def RMSprop(network, data, config,beta = 0.9,epsilon = 1e-8) :\n",
        "    \n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        "    \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "    v_dw, v_db = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    v_dw.reverse()\n",
        "    v_db.reverse()\n",
        "\n",
        "    for j in range(len(v_db)) :\n",
        "      v_db[j] = v_db[j].flatten()\n",
        "\n",
        "    for i in range(num_epochs) :\n",
        "      dw = []\n",
        "      db = []\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        X = X_train[k:k+batch_size]\n",
        "        Y = Y_train[k:k+batch_size]\n",
        "        temp = network.self_grad_wandb(X,Y,activation_func)  \n",
        "\n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "\n",
        "        for j in range(len(dw)) :\n",
        "          v_dw[j] *= beta\n",
        "          v_dw[j] += (1-beta)*(dw[j]**2) \n",
        "          dw[j] *= eta/np.sqrt(v_dw[j]+epsilon)\n",
        "          v_db[j] *= beta\n",
        "          v_db[j] += (1-beta)*(db[j]**2) \n",
        "          db[j] *= eta/np.sqrt(v_db[j]+epsilon)\n",
        "\n",
        "        network.update_vals(dw,db, lambda_)\n",
        "\n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def adam(network, data, config, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        " \n",
        "   \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "\n",
        "    m_w, m_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "    v_w, v_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    m_w.reverse()\n",
        "    m_b.reverse()\n",
        "    v_w.reverse()\n",
        "    v_b.reverse()\n",
        "    for j in range(len(m_b)):\n",
        "      m_b[j], v_b[j] = m_b[j].flatten(), v_b[j].flatten() \n",
        "    \n",
        "    t = 0\n",
        "    for i in range(num_epochs):\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        \n",
        "        t += 1\n",
        "        \n",
        "        X = X_train[k: k+batch_size]\n",
        "        Y = Y_train[k: k+batch_size]\n",
        "        \n",
        "        temp = network.self_grad_wandb(X, Y, activation_func)\n",
        "        \n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "        \n",
        "        for j in range(len(dw)):\n",
        "          \n",
        "          m_w[j] = beta1 * m_w[j] + (1 - beta1) * dw[j]\n",
        "          m_b[j] = beta1 * m_b[j] + (1 - beta1) * db[j]\n",
        "          \n",
        "          v_w[j] = beta2 * v_w[j] + (1 - beta2) * dw[j] * dw[j]\n",
        "          v_b[j] = beta2 * v_b[j] + (1 - beta2) * db[j] * db[j]\n",
        "                 \n",
        "          m_w[j] = m_w[j] *((1-beta1**int(t))/ (1-beta1**int(t+1)))\n",
        "          m_b[j] = m_b[j] *((1-beta1**int(t))/ (1-beta1**int(t+1)))\n",
        "          \n",
        "          v_w[j] = v_w[j]*((1-beta2**int(t))/ (1-beta2**int(t+1)))\n",
        "          v_b[j] = v_b[j]*((1-beta2**int(t))/ (1-beta2**int(t+1)))\n",
        "          \n",
        "          dw[j] = eta * m_w[j] / (epsilon + np.sqrt( v_w[j]))\n",
        "          db[j] = eta * m_b[j] / ( epsilon + np.sqrt(v_b[j]))\n",
        "\n",
        "         \n",
        "        network.update_vals(dw, db, lambda_)\n",
        "        \n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      })\n",
        "\n",
        "    \n",
        "  @staticmethod\n",
        "  def nadam(network, data, config, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        " \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "    \n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "\n",
        "    m_w, m_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "    v_w, v_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    m_w.reverse()\n",
        "    m_b.reverse()\n",
        "    v_w.reverse()\n",
        "    v_b.reverse()\n",
        "    for j in range(len(m_b)):\n",
        "      m_b[j], v_b[j] = m_b[j].flatten(), v_b[j].flatten() \n",
        "    \n",
        "    t = 0\n",
        "    for i in range(num_epochs):\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        \n",
        "        t += 1\n",
        "        \n",
        "        X = X_train[k: k+batch_size]\n",
        "        Y = Y_train[k: k+batch_size]\n",
        "        \n",
        "        temp = network.self_grad_wandb(X, Y, activation_func)\n",
        "        \n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "        \n",
        "        for j in range(len(dw)):\n",
        "          \n",
        "          m_w[j] = beta1 * m_w[j] + (1 - beta1) * dw[j]\n",
        "          m_b[j] = beta1 * m_b[j] + (1 - beta1) * db[j]\n",
        "          \n",
        "          v_w[j] = beta2 * v_w[j] + (1 - beta2) * dw[j] * dw[j]\n",
        "          v_b[j] = beta2 * v_b[j] + (1 - beta2) * db[j] * db[j]\n",
        "                 \n",
        "          m_w[j] = m_w[j] *((1-beta1**int(t))/ (1-beta1**int(t+1)))\n",
        "          m_b[j] = m_b[j] *((1-beta1**int(t))/ (1-beta1**int(t+1)))\n",
        "          \n",
        "          v_w[j] = v_w[j]*((1-beta2**int(t))/ (1-beta2**int(t+1)))\n",
        "          v_b[j] = v_b[j]*((1-beta2**int(t))/ (1-beta2**int(t+1)))\n",
        "          \n",
        "          \n",
        "          dw[j] = eta * (beta1*m_w[j] + (1 - beta1)*dw[j]) / (epsilon + np.sqrt( v_w[j]))\n",
        "          db[j] = eta * (beta1*m_b[j] + (1 - beta1)*db[j]) / ( epsilon + np.sqrt(v_b[j]))\n",
        "        \n",
        "        network.update_vals(dw, db, lambda_)\n",
        "        \n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKDlM0FUKutJ"
      },
      "source": [
        "def run_callback(network,data,config) :\n",
        "    \n",
        "    activation_func = config['activation']\n",
        "    \n",
        "    X_train = data['train']['X']\n",
        "    Y_train = data['train']['Y']\n",
        "\n",
        "    X_val = data['val']['X']\n",
        "    Y_val = data['val']['Y']\n",
        "\n",
        "    train_loss = 0\n",
        "    train_count = 0\n",
        "    train_sq_error = 0\n",
        "    \n",
        "    temp = network.predict(X_train,activation_func)\n",
        "    train_count = np.sum(temp['Y'].reshape(Y_train.shape)==Y_train)\n",
        "    \n",
        "    Y_pred = np.array(temp['Y_hat'].T)\n",
        "    train_loss = np.sum(-np.log(Y_pred[np.arange(len(X_train)),Y_train]))\n",
        "    E = np.zeros(Y_pred.shape)\n",
        "    E[np.arange(len(X_train)),Y_train] = 1\n",
        "    train_sq_error = np.sum((E-Y_pred)**2)\n",
        "\n",
        "    val_loss = 0\n",
        "    val_count = 0\n",
        "    val_sq_error = 0\n",
        "    \n",
        "    temp = network.predict(X_val, activation_func)\n",
        "    val_count = np.sum(temp['Y'].reshape(Y_val.shape)==Y_val)\n",
        "    \n",
        "    Y_pred = np.array(temp['Y_hat'].T)\n",
        "    val_loss = np.sum(-np.log(Y_pred[np.arange(len(X_val)),Y_val]))\n",
        "    E = np.zeros(Y_pred.shape)\n",
        "    E[np.arange(len(X_val)),Y_val] = 1\n",
        "    val_sq_error = np.sum((E-Y_pred)**2)\n",
        "    \n",
        "    return  {\n",
        "        'loss': {\n",
        "            'train' : train_loss / len(X_train),\n",
        "            'val' : val_loss / len(X_val)\n",
        "        },\n",
        "        'accuracy': {\n",
        "            'train': train_count / len(X_train),\n",
        "            'val': val_count / len(X_val)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9IjksI6O6wE"
      },
      "source": [
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'val_loss',\n",
        "      'goal': 'minimize'\n",
        "     } ,\n",
        "\n",
        "    'parameters': {\n",
        "        'num_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'num_hidden_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.005, 0.0005]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-3, 1e-4, 1e-2]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'NAG', 'RMSprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 32, 16]\n",
        "        },\n",
        "        'weights_initializer': {\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid', 'tanh', 'relu']\n",
        "        }        \n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F91KWHHABHv5"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='test8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcXWo9omoU8q"
      },
      "source": [
        "sweep_id = 'qkd49l9s'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhn-A6mcO6wF",
        "outputId": "2641d894-693d-4777-bd84-5b035e6f6c36"
      },
      "source": [
        "pprint.pprint(sweep_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'method': 'bayes',\n",
            " 'metric': {'goal': 'maximize', 'name': 'val_acc'},\n",
            " 'parameters': {'activation': {'values': ['sigmoid']},\n",
            "                'batch_size': {'values': [16, 32]},\n",
            "                'hidden_layer_size': {'values': [64]},\n",
            "                'lr': {'values': [0.001]},\n",
            "                'num_epochs': {'values': [10]},\n",
            "                'num_hidden_layers': {'values': [3]},\n",
            "                'optimizer': {'values': ['RMSprop']},\n",
            "                'weight_decay': {'values': [0]},\n",
            "                'weights_initializer': {'values': ['xavier']}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qml5BFsMO6wF"
      },
      "source": [
        "class sweep_module:\n",
        "  @staticmethod\n",
        "  def train(config=None):\n",
        "\n",
        "    with wandb.init(config):\n",
        "      \n",
        "      config = wandb.config\n",
        "      wandb.run.name = 'ac:'+config['activation'][:3]+'_opt:'+config['optimizer'][:4]+'_hl:'+str(config['num_hidden_layers'])+':'+str(config['hidden_layer_size'])\n",
        "      \n",
        "      nn_shape = set_nn_shape(False, config['num_hidden_layers'] , config['hidden_layer_size'])\n",
        "      \n",
        "      network = neural_network(nn_shape, config['weights_initializer'])\n",
        "      \n",
        "      getattr(optimizer, config['optimizer'])(network, data, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCYYRxwNO6wF"
      },
      "source": [
        "# performing the sweep\n",
        "wandb.agent(sweep_id, sweep_module.train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}