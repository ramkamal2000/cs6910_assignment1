{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try_dl_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "761045a0087246b18ab5ad30dfa2b21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ad9ee386314b4efe95d5e403d05dd6de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a9c511f171d148ae8a60060db79d0d31",
              "IPY_MODEL_dc19a6b0f3444ebea5143c44b3cd3638"
            ]
          }
        },
        "ad9ee386314b4efe95d5e403d05dd6de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9c511f171d148ae8a60060db79d0d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0190965b67240a5885a51cf9dc44163",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3375,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3375,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c54efae36c54ee3879926645832798d"
          }
        },
        "dc19a6b0f3444ebea5143c44b3cd3638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d96301313c544fefab941ff6d2d75b42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3375/3375 [00:39&lt;00:00, 84.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f554f4d1d364daa985a791f65f1dbed"
          }
        },
        "a0190965b67240a5885a51cf9dc44163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c54efae36c54ee3879926645832798d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d96301313c544fefab941ff6d2d75b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f554f4d1d364daa985a791f65f1dbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "475a8bd6a54a4504a8218fce59289cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97151dc6dcfd4dfbbba8472cb8c9585a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3be76f1894c04049adf068aab3d27e43",
              "IPY_MODEL_a7b9e78189e74af982d58f4c5d244f03"
            ]
          }
        },
        "97151dc6dcfd4dfbbba8472cb8c9585a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3be76f1894c04049adf068aab3d27e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad3fa7a799614bcda5cd715b7dfd3fa1",
            "_dom_classes": [],
            "description": " 35%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3375,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1190,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3dbd3696fb5405e940a608cf1cbe7c7"
          }
        },
        "a7b9e78189e74af982d58f4c5d244f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_855bbc0feab543b7b2a9110d6678eb89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1190/3375 [00:20&lt;00:38, 56.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e3639f7f1434defba9de6044264852d"
          }
        },
        "ad3fa7a799614bcda5cd715b7dfd3fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3dbd3696fb5405e940a608cf1cbe7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "855bbc0feab543b7b2a9110d6678eb89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e3639f7f1434defba9de6044264852d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_k91V7UYQl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687bfbdc-847f-494e-b912-88ed61b1991b"
      },
      "source": [
        "!pip install wandb\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from tqdm.auto import tqdm\n",
        "import tensorflow as tf\n",
        "import wandb\n",
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjPTg70-YXjM"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESi2E1-zC6GV"
      },
      "source": [
        "def load_fashion_mnist(return_images=False, test=False):\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  train_shuffler = np.random.shuffle(np.arange(60000))\n",
        "  x_train, y_train = x_train[train_shuffler][0], y_train[train_shuffler][0]\n",
        "\n",
        "  test_shuffler = np.random.shuffle(np.arange(10000))\n",
        "  x_test, y_test = x_test[test_shuffler][0], y_test[test_shuffler][0]\n",
        "\n",
        "  x_train = np.array(x_train/255).astype('float32')\n",
        "  x_test = np.array(x_test/255).astype('float32')\n",
        "\n",
        "  if test==False:\n",
        "    x_train, x_val = x_train[:54000], x_train[54000:]\n",
        "    y_train, y_val = y_train[:54000], y_train[54000:]\n",
        "\n",
        "    if (return_images==False):\n",
        "      return {\n",
        "          'train': {\n",
        "              'X': x_train.reshape([-1, 784]),\n",
        "              'Y': y_train.reshape([54000])\n",
        "          },\n",
        "          'val': {\n",
        "              'X': x_val.reshape([-1, 784]),\n",
        "              'Y': y_val.reshape([6000])\n",
        "          },\n",
        "          'test': {\n",
        "              'X': x_test.reshape([-1, 784]),\n",
        "              'Y': y_test.reshape([10000])\n",
        "          }\n",
        "    }\n",
        "\n",
        "    else :\n",
        "      return {\n",
        "        'train': {\n",
        "              'X': x_train,\n",
        "              'Y': y_train\n",
        "        },\n",
        "        'val': {\n",
        "              'X': x_val,\n",
        "              'Y': y_val\n",
        "        },\n",
        "        'test': {\n",
        "              'X': x_test,\n",
        "              'Y': y_test\n",
        "        }\n",
        "      }\n",
        "\n",
        "  else:\n",
        "    print('train = Old Train + Old Val', 'val = Old Test', sep='\\n')\n",
        "    return {\n",
        "        'train': {\n",
        "            'X': x_train.reshape([-1, 784]),\n",
        "            'Y': y_train.reshape([60000])\n",
        "        },\n",
        "        'val': {\n",
        "            'X': x_test.reshape([-1, 784]),\n",
        "            'Y': y_test.reshape([10000])\n",
        "        }\n",
        "    }\n",
        "\n",
        "data = load_fashion_mnist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-EXaU4oiZ_"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23nXB0VBojt0"
      },
      "source": [
        "class neural_network:\n",
        "\n",
        "  # constructor function - initializes weights\n",
        "  def __init__(self, dict_layers, initializer):\n",
        "\n",
        "    self.weights_list = []\n",
        "    self.biases_list = []\n",
        "    self.dict_layers= dict_layers\n",
        "\n",
        "    self.weights_list, self.biases_list = wandb_initializer(dict_layers, self.weights_list, self.biases_list, initializer)\n",
        "\n",
        "  # function to compute forward propogation\n",
        "  def forward_prop(self, W, b, X, Y, activation_func):\n",
        "    '''\n",
        "    This fuction implements forward propagation\n",
        "\n",
        "      forward propagation :\n",
        "      A_i = W_i*H_(i-1) + b_i\n",
        "      H_i = activation_func(A_i)\n",
        "\n",
        "      Output = softmax(A_L)\n",
        "\n",
        "    Returns :\n",
        "    Output (Y_hat) , A , H\n",
        "    '''\n",
        "    A = []\n",
        "    H = []\n",
        "    \n",
        "    H_pre = X\n",
        "    \n",
        "    L = self.dict_layers['num_hidden_layers']\n",
        "\n",
        "    for i in range(L) :\n",
        "      A.append(W[i] @ H_pre + b[i])\n",
        "      H_pre = getattr(activation, activation_func)(A[i])\n",
        "      H.append(H_pre)\n",
        "    \n",
        "    A.append(W[L] @ H_pre + b[L])\n",
        "    \n",
        "    Y_hat = activation.softmax(A[L])\n",
        "    \n",
        "    return {\n",
        "        'A' : A,\n",
        "        'H' : H,\n",
        "        'Y_hat' : Y_hat\n",
        "    }\n",
        "\n",
        "  # helper function to perform forward propogation \n",
        "  def self_forward_prop(self, X, Y, activation_func) :\n",
        "\n",
        "    temp = self.forward_prop(self.weights_list,self.biases_list, X, Y, activation_func)\n",
        "    return temp\n",
        "\n",
        "  # function to perform backward propogration\n",
        "  def back_prop(self, W, b, A, H, Y_hat, X, Y,activation_func):\n",
        "    '''\n",
        "    This function implements backpropogation :\n",
        "      \n",
        "      Backprop :\n",
        "      L = cross-entropy loss\n",
        "\n",
        "      δL/δW_i = (δL/δA_i)(H_(i-1))\n",
        "      δL/δb = δL/δA_i\n",
        "      δL/δH_(i-1) = (W_i)^T(δL/δA)\n",
        "      δL/δA_(i-1) = δL/δH_(i-1) ⊙ g'(H_(i-1))\n",
        "\n",
        "    Inputs : Y_hat - Y predicted, Y - true Y , X - train data , A,H - from forward propogation\n",
        "\n",
        "    Returns : δL/δW , δL/δb\n",
        "    '''\n",
        "\n",
        "    batch_size = len(Y)\n",
        "    \n",
        "    del_w = []\n",
        "    del_b = []\n",
        "    L = self.dict_layers['num_hidden_layers']\n",
        "    \n",
        "    E = np.zeros(Y_hat.shape)\n",
        "    E[Y,np.arange(batch_size)] = 1\n",
        "    \n",
        "    grad_A = -(E - Y_hat)\n",
        "\n",
        "    for i in range(L,-1,-1) :\n",
        "\n",
        "      temp1 = grad_A.reshape(-1,batch_size)\n",
        "      \n",
        "      if i==0 :\n",
        "        temp2 = X.T\n",
        "      else :\n",
        "        temp2 = H[i-1].reshape((batch_size ,-1))\n",
        "\n",
        "      del_w.append(temp1 @ temp2/batch_size)\n",
        "      del_b.append(grad_A/batch_size)\n",
        "\n",
        "      if(i!=0) :\n",
        "        grad_H = W[i].T @ grad_A      \n",
        "        grad_A = grad_H * getattr(activation,activation_func+'_der')(H[i-1])\n",
        "\n",
        "    for j in range(len(del_b)) :\n",
        "       del_b[j] = np.sum(del_b[j],axis=1)\n",
        "\n",
        "    return {\n",
        "        'dw' : del_w,\n",
        "        'db' : del_b\n",
        "    }\n",
        "\n",
        "  # helper function to perform backward propogation\n",
        "  def self_back_prop(self, A, H, Y_hat, X, Y,activation_func) :\n",
        "    temp = self.back_prop(self.weights_list,self.biases_list, A, H, Y_hat, X, Y, activation_func)\n",
        "    return temp\n",
        "\n",
        "  #  function to compute gradient\n",
        "  def grad_wandb(self, W, b, X, Y,activation_func):\n",
        "    ''' \n",
        "    Function to find gradient of Cross entropy loss with respect to given W and b\n",
        "\n",
        "    Arguments : W,B and data (X,Y)\n",
        "    Returns : Returns : δL/δW , δL/δb\n",
        "    '''\n",
        "\n",
        "    X = X.T.reshape((784,-1))\n",
        "    \n",
        "    temp = self.forward_prop(W, b, X, Y, activation_func)\n",
        "    temp2 = self.back_prop(W, b, temp['A'], temp['H'], temp['Y_hat'], X, Y, activation_func)\n",
        "\n",
        "    return {\n",
        "        'dw' : temp2['dw'],\n",
        "        'db' : temp2['db']\n",
        "    }\n",
        "\n",
        "  # helper function to compute gradient\n",
        "  def self_grad_wandb(self, X, Y, activation_func) :\n",
        "    temp = self.grad_wandb(self.weights_list, self.biases_list, X, Y,activation_func)\n",
        "    return temp\n",
        "\n",
        "  # function to compute predictions\n",
        "  def predict(self, X, activation_func):\n",
        "    '''\n",
        "    Function to take X and give the prediction of the network on this X\n",
        "    '''\n",
        "    X = X.T.reshape((784,-1))\n",
        "    temp = self.forward_prop(self.weights_list,self.biases_list, X, 0, activation_func)\n",
        "    return {\n",
        "      'Y' : np.argmax(temp['Y_hat'],axis=0),\n",
        "      'Y_hat' : temp['Y_hat']\n",
        "    }\n",
        "\n",
        "  # function to update weights and biases\n",
        "  def update_vals(self, dw, db, wd) :\n",
        "    '''\n",
        "    Functions to update parameters of the network\n",
        "    Arguments : change in weights(dw) , change in biases (db) , Regularization parameter : wd\n",
        "    '''\n",
        "    L = len(self.weights_list)\n",
        "    for i in range(L) :\n",
        "      self.weights_list[i] =self.weights_list[i] - dw[L-i-1].reshape(self.weights_list[i].shape) - wd * self.weights_list[i]\n",
        "\n",
        "    #for i in range(len(self.biases_list)) :\n",
        "      self.biases_list[i] =self.biases_list[i] - db[L-i-1].reshape(self.biases_list[i].shape)  \n",
        "##################################################################################\n",
        "class activation:\n",
        "  \n",
        "  @staticmethod\n",
        "  def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "  \n",
        "  @staticmethod\n",
        "  def relu(z):\n",
        "    return (z>0) * z\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_der(z) :\n",
        "    return z * (1-z)\n",
        "  \n",
        "  @staticmethod\n",
        "  def relu_der(z) :\n",
        "    return (z>0)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_der(z):\n",
        "    return 1 - z*z\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x,axis=0)\n",
        "\n",
        "##################################################################################\n",
        "def set_nn_shape(verbose=True, num_hidden_layers=-1, hidden_layer_size=-1):\n",
        "\n",
        "  input_layer_size = 784\n",
        "  hidden_layer_size = hidden_layer_size\n",
        "  num_hidden_layers = num_hidden_layers\n",
        "  output_layer_size = 10\n",
        "  \n",
        "\n",
        "  if (verbose):\n",
        "    print(\"\\nNumber Of Hidden Layers:\")\n",
        "    num_hidden_layers = int(input())\n",
        "\n",
        "    print(\"\\nSize Of Each Hidden Layer:\")\n",
        "    hidden_layer_size = int(input())\n",
        "\n",
        "    print(f\"\\nThe Neural Network Has {num_hidden_layers+2} Layers In Total!\")\n",
        "  \n",
        "  return {\"input_layer_size\": input_layer_size, \"hidden_layer_size\": hidden_layer_size, \"output_layer_size\": output_layer_size, \"num_hidden_layers\": num_hidden_layers}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIFMpZQNItG3"
      },
      "source": [
        "def wandb_initializer(nn_shape, weights_list, biases_list, type='random', mu = 0, sigma = 1):\n",
        "  \n",
        "  # random initialization\n",
        "  if (type=='random'):\n",
        "    initializer = tf.keras.initializers.TruncatedNormal(mean=mu, stddev=sigma)\n",
        "  \n",
        "  # xavier initialization\n",
        "  elif (type=='xavier'):\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "  weights_list.append(initializer(shape=(nn_shape['hidden_layer_size'], nn_shape['input_layer_size'])).numpy())\n",
        "  biases_list.append(initializer(shape=(nn_shape['hidden_layer_size'], 1)).numpy())\n",
        "  for i in range(nn_shape['num_hidden_layers'] - 1):\n",
        "    weights_list.append(initializer(shape=(nn_shape['hidden_layer_size'], nn_shape['hidden_layer_size'])).numpy())\n",
        "    biases_list.append(initializer(shape=(nn_shape['hidden_layer_size'], 1)).numpy())\n",
        "\n",
        "  weights_list.append(initializer(shape=(nn_shape['output_layer_size'], nn_shape['hidden_layer_size'])).numpy())\n",
        "  biases_list.append(initializer(shape=(nn_shape['output_layer_size'], 1)).numpy())\n",
        "\n",
        "  return weights_list, biases_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVpF16BtXqB_"
      },
      "source": [
        "class optimizer:\n",
        "\n",
        "  @staticmethod\n",
        "  def sgd(network, data, config):\n",
        "    '''\n",
        "    This function implements gradient descent\n",
        "      Gradient descent :\n",
        "      param = param - eta*δL/δparam\n",
        "\n",
        "    '''\n",
        "\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        " \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        X = X_train[k: k+batch_size]\n",
        "        Y = Y_train[k: k+batch_size]\n",
        "        temp = network.self_grad_wandb(X, Y, activation_func)         \n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "        for dd in dw :\n",
        "          dd*= eta\n",
        "        for dd in db :\n",
        "          dd*=eta\n",
        "\n",
        "        network.update_vals(dw, db, lambda_)\n",
        "    \n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def momentum(network, data, config,gamma = 0.9) :\n",
        "    '''\n",
        "    This function implements momentum based Gradient descent\n",
        "     Momentum :\n",
        "     update_t = gamma*update_(t-1) + eta*δL/δparam\n",
        "     param = param - update_t\n",
        "\n",
        "    '''\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        "    \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "    dw, db = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    dw.reverse()\n",
        "    db.reverse()\n",
        "\n",
        "    for j in range(len(db)) :\n",
        "      db[j] = db[j].flatten()\n",
        "\n",
        "    for i in range(num_epochs) :\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        X = X_train[k:k+batch_size]\n",
        "        Y = Y_train[k:k+batch_size]\n",
        "        temp = network.self_grad_wandb(X,Y,activation_func)       \n",
        "        for j in range(len(dw)) :\n",
        "          dw[j] += eta*temp['dw'][j]\n",
        "          db[j] += eta*temp['db'][j]\n",
        "\n",
        "        network.update_vals(dw,db, lambda_)\n",
        "        for dd in db :\n",
        "          dd*=gamma\n",
        "        for dd in dw :\n",
        "          dd*=gamma\n",
        "\n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "        \n",
        "  @staticmethod\n",
        "  def NAG(network, data, config,gamma = 0.9) :\n",
        "    '''\n",
        "    This function implements Nesterov Accelerated Gradient descent\n",
        "     NAG :\n",
        "     param_lookahead = param - gamma*upadte_(t-1)\n",
        "     update_t = gamma*update_(t-1) + eta*δL/δparam_lookahead\n",
        "     param = param - update_t\n",
        "\n",
        "    '''\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        "    \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    \n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "    v_dw, v_db = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    v_dw.reverse()\n",
        "    v_db.reverse()\n",
        "\n",
        "    for j in range(len(v_db)) :\n",
        "      v_db[j] = v_db[j].flatten()\n",
        "\n",
        "    for i in range(num_epochs) :\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        for j in range(len(v_dw)) :\n",
        "          v_dw[j] = gamma*v_dw[j]\n",
        "          v_db[j] = gamma*v_db[j]\n",
        "\n",
        "        X = X_train[k:k+batch_size]\n",
        "        Y = Y_train[k:k+batch_size]\n",
        "        \n",
        "        W = network.weights_list.copy()\n",
        "        B = network.biases_list.copy()\n",
        "\n",
        "        L = len(W)\n",
        "        for j in range(L) :\n",
        "          W[j] -= v_dw[L-j-1]\n",
        "          B[j] -= v_db[L-j-1].reshape(B[j].shape)\n",
        "\n",
        "        temp = network.grad_wandb(W,B,X,Y,activation_func)  \n",
        "\n",
        "        for j in range(len(v_dw)) :\n",
        "          v_dw[j] += eta*temp['dw'][j]\n",
        "          v_db[j] += eta*temp['db'][j]\n",
        "\n",
        "\n",
        "        network.update_vals(v_dw,v_db,lambda_)\n",
        "\n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def RMSprop(network, data, config,beta = 0.9,epsilon = 1e-8) :\n",
        "    '''\n",
        "    This function implements RMS prop\n",
        "     RMSprop :\n",
        "     v_t = beat*v_(t-1) + (1-beta)*(δL/δparam)**2\n",
        "     param = param - (eta/sqrt(epsilon + v_t))*δL/δparam\n",
        "\n",
        "    '''\n",
        "    \n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        "    \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "\n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "    v_dw, v_db = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    v_dw.reverse()\n",
        "    v_db.reverse()\n",
        "\n",
        "    for j in range(len(v_db)) :\n",
        "      v_db[j] = v_db[j].flatten()\n",
        "\n",
        "    for i in range(num_epochs) :\n",
        "      dw = []\n",
        "      db = []\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        X = X_train[k:k+batch_size]\n",
        "        Y = Y_train[k:k+batch_size]\n",
        "        temp = network.self_grad_wandb(X,Y,activation_func)  \n",
        "\n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "\n",
        "        for j in range(len(dw)) :\n",
        "          v_dw[j] *= beta\n",
        "          v_dw[j] += (1-beta)*(dw[j]**2) \n",
        "          dw[j] *= eta/np.sqrt(v_dw[j]+epsilon)\n",
        "          v_db[j] *= beta\n",
        "          v_db[j] += (1-beta)*(db[j]**2) \n",
        "          db[j] *= eta/np.sqrt(v_db[j]+epsilon)\n",
        "\n",
        "        network.update_vals(dw,db, lambda_)\n",
        "\n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      }) \n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def adam(network, data, config, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    '''\n",
        "    This function implements Adam\n",
        "    Adam is similar to RMS prop,but with momentum\n",
        "\n",
        "    '''\n",
        "\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        " \n",
        "   \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "\n",
        "    m_w, m_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "    v_w, v_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    m_w.reverse()\n",
        "    m_b.reverse()\n",
        "    v_w.reverse()\n",
        "    v_b.reverse()\n",
        "    for j in range(len(m_b)):\n",
        "      m_b[j], v_b[j] = m_b[j].flatten(), v_b[j].flatten() \n",
        "    \n",
        "    t = 0\n",
        "    eta = eta/(1-beta1)\n",
        "    for i in range(num_epochs):\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        \n",
        "        t += 1\n",
        "        \n",
        "        X = X_train[k: k+batch_size]\n",
        "        Y = Y_train[k: k+batch_size]\n",
        "        \n",
        "        temp = network.self_grad_wandb(X, Y, activation_func)\n",
        "        \n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "        \n",
        "        for j in range(len(dw)):\n",
        "          \n",
        "          m_w[j] = beta1 * m_w[j] + (1 - beta1) * dw[j]\n",
        "          m_b[j] = beta1 * m_b[j] + (1 - beta1) * db[j]\n",
        "          \n",
        "          v_w[j] = beta2 * v_w[j] + (1 - beta2) * dw[j] * dw[j]\n",
        "          v_b[j] = beta2 * v_b[j] + (1 - beta2) * db[j] * db[j]\n",
        "                 \n",
        "          m_w_hat = m_w[j] *((1-beta1)/ (1-beta1**int(t+1)))\n",
        "          m_b_hat = m_b[j] *((1-beta1)/ (1-beta1**int(t+1)))\n",
        "          \n",
        "          v_w_hat = v_w[j]*((1-beta2)/ (1-beta2**int(t+1)))\n",
        "          v_b_hat = v_b[j]*((1-beta2)/ (1-beta2**int(t+1)))\n",
        "          \n",
        "          dw[j] = eta * m_w_hat / (epsilon + np.sqrt( v_w_hat))\n",
        "          db[j] = eta * m_b_hat / ( epsilon + np.sqrt(v_b_hat))\n",
        "\n",
        "         \n",
        "        network.update_vals(dw, db, lambda_)\n",
        "        \n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      })\n",
        "\n",
        "    \n",
        "  @staticmethod\n",
        "  def nadam(network, data, config, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    '''\n",
        "    This function implements Nadam.\n",
        "    Just like Adam is RMS prop with momentum,\n",
        "    Nadam is RMS prop with Nesterov\n",
        "\n",
        "    '''\n",
        "\n",
        "    num_epochs, batch_size = config['num_epochs'], config['batch_size']\n",
        "    eta, lambda_ = config['lr'], config['weight_decay']\n",
        "    initializer, activation_func = config['weights_initializer'], config['activation']\n",
        " \n",
        "    X_train, Y_train = data['train']['X'], data['train']['Y']\n",
        "    num_examples = len(X_train)\n",
        "    \n",
        "    nn_shape = set_nn_shape(False, config['num_hidden_layers'], config['hidden_layer_size'])\n",
        "\n",
        "    m_w, m_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "    v_w, v_b = wandb_initializer(nn_shape, [], [], 'random', 0, 0)\n",
        "\n",
        "    m_w.reverse()\n",
        "    m_b.reverse()\n",
        "    v_w.reverse()\n",
        "    v_b.reverse()\n",
        "    for j in range(len(m_b)):\n",
        "      m_b[j], v_b[j] = m_b[j].flatten(), v_b[j].flatten() \n",
        "    \n",
        "    t = 0\n",
        "    eta = eta/(1-beta1)\n",
        "    for i in range(num_epochs):\n",
        "      for k in tqdm(range(0, len(X_train), batch_size)) :\n",
        "        \n",
        "        t += 1\n",
        "        \n",
        "        X = X_train[k: k+batch_size]\n",
        "        Y = Y_train[k: k+batch_size]\n",
        "        \n",
        "        temp = network.self_grad_wandb(X, Y, activation_func)\n",
        "        \n",
        "        dw = temp['dw']\n",
        "        db = temp['db']\n",
        "        \n",
        "        for j in range(len(dw)):\n",
        "          \n",
        "          m_w[j] = beta1 * m_w[j] + (1 - beta1) * dw[j]\n",
        "          m_b[j] = beta1 * m_b[j] + (1 - beta1) * db[j]\n",
        "          \n",
        "          v_w[j] = beta2 * v_w[j] + (1 - beta2) * dw[j] * dw[j]\n",
        "          v_b[j] = beta2 * v_b[j] + (1 - beta2) * db[j] * db[j]\n",
        "                 \n",
        "          m_w_hat = m_w[j] *((1-beta1)/ (1-beta1**int(t+1)))\n",
        "          m_b_hat = m_b[j] *((1-beta1)/ (1-beta1**int(t+1)))\n",
        "          \n",
        "          v_w_hat = v_w[j]*((1-beta2)/ (1-beta2**int(t+1)))\n",
        "          v_b_hat = v_b[j]*((1-beta2)/ (1-beta2**int(t+1)))\n",
        "          \n",
        "          \n",
        "          dw[j] = eta * (beta1*m_w_hat + (1 - beta1)*dw[j]) / (epsilon + np.sqrt( v_w_hat))\n",
        "          db[j] = eta * (beta1*m_b_hat + (1 - beta1)*db[j]) / ( epsilon + np.sqrt(v_b_hat))\n",
        "        \n",
        "        network.update_vals(dw, db, lambda_)\n",
        "        \n",
        "      report = run_callback(network, data, config) \n",
        "        \n",
        "      wandb.log({\n",
        "            'batch_size': config.batch_size, \n",
        "            'val_loss' : report['loss']['val'], \n",
        "            'train_loss': report['loss']['train'],\n",
        "            'train_acc': report['accuracy']['train'],\n",
        "            'val_acc': report['accuracy']['val']  \n",
        "      })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKDlM0FUKutJ"
      },
      "source": [
        "def run_callback(network,data,config) :\n",
        "    '''\n",
        "    This function is used to calculate accuracy and loss\n",
        "    '''\n",
        "    activation_func = config['activation']\n",
        "    \n",
        "    X_train = data['train']['X']\n",
        "    Y_train = data['train']['Y']\n",
        "\n",
        "    X_val = data['val']['X']\n",
        "    Y_val = data['val']['Y']\n",
        "\n",
        "    train_loss = 0\n",
        "    train_count = 0\n",
        "    train_sq_error = 0\n",
        "    \n",
        "    temp = network.predict(X_train,activation_func)\n",
        "    train_count = np.sum(temp['Y'].reshape(Y_train.shape)==Y_train)\n",
        "    \n",
        "    Y_pred = np.array(temp['Y_hat'].T)\n",
        "    train_loss = np.sum(-np.log(Y_pred[np.arange(len(X_train)),Y_train]))\n",
        "    E = np.zeros(Y_pred.shape)\n",
        "    E[np.arange(len(X_train)),Y_train] = 1\n",
        "    train_sq_error = np.sum((E-Y_pred)**2)\n",
        "\n",
        "    val_loss = 0\n",
        "    val_count = 0\n",
        "    val_sq_error = 0\n",
        "    \n",
        "    temp = network.predict(X_val, activation_func)\n",
        "    val_count = np.sum(temp['Y'].reshape(Y_val.shape)==Y_val)\n",
        "    \n",
        "    Y_pred = np.array(temp['Y_hat'].T)\n",
        "    val_loss = np.sum(-np.log(Y_pred[np.arange(len(X_val)),Y_val]))\n",
        "    E = np.zeros(Y_pred.shape)\n",
        "    E[np.arange(len(X_val)),Y_val] = 1\n",
        "    val_sq_error = np.sum((E-Y_pred)**2)\n",
        "    \n",
        "    return  {\n",
        "        'loss': {\n",
        "            'train' : train_loss / len(X_train),\n",
        "            'val' : val_loss / len(X_val)\n",
        "        },\n",
        "        'accuracy': {\n",
        "            'train': train_count / len(X_train),\n",
        "            'val': val_count / len(X_val)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9IjksI6O6wE"
      },
      "source": [
        "sweep_config = {\n",
        "    'method' : 'grid',\n",
        "\n",
        "    'parameters': {\n",
        "        'num_epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'num_hidden_layers': {\n",
        "            'values': [5]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [256]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-4, 5e-4, 1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['momentum', 'RMSprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 16]\n",
        "        },\n",
        "        'weights_initializer': {\n",
        "            'values': ['xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu']\n",
        "        }        \n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F91KWHHABHv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4777a38-f35a-422a-b102-79cb598c8914"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='mnist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhn-A6mcO6wF",
        "outputId": "6eb0d2aa-3cbe-44b0-f5f7-3625899d3aff"
      },
      "source": [
        "pprint.pprint(sweep_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qml5BFsMO6wF"
      },
      "source": [
        "class sweep_module:\n",
        "  @staticmethod\n",
        "  def train(config=None):\n",
        "\n",
        "    with wandb.init(config):\n",
        "      \n",
        "      config = wandb.config\n",
        "      wandb.run.name = 'ac:'+config['activation'][:3]+'_opt:'+config['optimizer'][:4]+'_hl:'+str(config['num_hidden_layers'])+':'+str(config['hidden_layer_size'])\n",
        "      \n",
        "      nn_shape = set_nn_shape(False, config['num_hidden_layers'] , config['hidden_layer_size'])\n",
        "      \n",
        "      network = neural_network(nn_shape, config['weights_initializer'])\n",
        "      \n",
        "      getattr(optimizer, config['optimizer'])(network, data, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCYYRxwNO6wF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "761045a0087246b18ab5ad30dfa2b21a",
            "ad9ee386314b4efe95d5e403d05dd6de",
            "a9c511f171d148ae8a60060db79d0d31",
            "dc19a6b0f3444ebea5143c44b3cd3638",
            "a0190965b67240a5885a51cf9dc44163",
            "4c54efae36c54ee3879926645832798d",
            "d96301313c544fefab941ff6d2d75b42",
            "9f554f4d1d364daa985a791f65f1dbed",
            "475a8bd6a54a4504a8218fce59289cb9",
            "97151dc6dcfd4dfbbba8472cb8c9585a",
            "3be76f1894c04049adf068aab3d27e43",
            "a7b9e78189e74af982d58f4c5d244f03",
            "ad3fa7a799614bcda5cd715b7dfd3fa1",
            "f3dbd3696fb5405e940a608cf1cbe7c7",
            "855bbc0feab543b7b2a9110d6678eb89",
            "0e3639f7f1434defba9de6044264852d"
          ]
        },
        "outputId": "bb928782-cb4f-47fb-d793-a3ae36557c67"
      },
      "source": [
        "# performing the sweep\n",
        "wandb.agent(sweep_id, sweep_module.train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}